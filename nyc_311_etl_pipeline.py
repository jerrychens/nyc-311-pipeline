# dags/nyc_311_etl_pipeline.py
# é€™ç‰ˆç¨‹å¼ç¢¼å¯ä»¥ç©©å®šæ”¯æ´ æ¯æ—¥è‡ªå‹•åŸ·è¡Œèˆ‡æ‰‹å‹• backfillï¼Œä¹Ÿå…·å‚™äº†å¸¸è¦‹ä¾‹å¤–è™•ç†çš„å½ˆæ€§ã€‚

from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import get_current_context
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from docker.types import Mount
from airflow.utils.dates import days_ago
from airflow.exceptions import AirflowSkipException
from datetime import datetime, timedelta
import os
import pandas as pd
import requests

# === DAG Configuration ===
default_args = {
    'owner': 'airflow',
    'retries': 0,
    'retry_delay': timedelta(seconds=5), # ç¸®çŸ­ retry é–“éš”
}
host_dbt_path = os.path.abspath("/Users/jerrychen/Documents/311/dbt")
PARTITION_TYPE = "week"  # å¯é¸ï¼š"day" / "week" / "month"


# === 1. æŠ“è³‡æ–™ ===
@task()
def fetch_data():
    context = get_current_context()
    execution_date = context["execution_date"]  # ç¢ºä¿æ˜¯ datetime ç‰©ä»¶
    
    base_url = "https://data.cityofnewyork.us/resource/erm2-nwe9.json"

    # è‹¥ç„¡å‚³å…¥ execution_dateï¼Œé è¨­ç‚ºç¾åœ¨æ™‚é–“ï¼ˆä¸»è¦æ˜¯æ‰‹å‹•åŸ·è¡Œæ™‚ç”¨ï¼‰
    if execution_date is None:
        execution_date = datetime.utcnow()

    # åˆ¤æ–·æ˜¯å¦ç‚ºç¬¬ä¸€æ¬¡åŸ·è¡Œï¼ˆå‡è¨­ start_date è¨­ç‚º days_ago(1)ï¼‰
    # å¦‚æžœæ˜¯ç¬¬ä¸€æ¬¡ï¼Œå°±æŠ“å–éŽåŽ»ä¸€æ•´å¹´çš„è³‡æ–™
    first_run = execution_date.date() == (days_ago(1)).date()
    if first_run:
        since = (execution_date - timedelta(days=365)).strftime('%Y-%m-%dT00:00:00')
    else:
        # å¦å‰‡åªæŠ“ç•¶å¤©è³‡æ–™ï¼ˆå–®æ—¥å¢žé‡ï¼‰
        since = execution_date.strftime('%Y-%m-%dT00:00:00')

    # æŠ“å–å€é–“çµå°¾ï¼ˆæ¬¡æ—¥ï¼‰
    until = (execution_date + timedelta(days=1)).strftime('%Y-%m-%dT00:00:00')

    # è¨­å®š API æŸ¥è©¢åƒæ•¸
    params = {
        #"$limit": 50000,
        "$where": f"created_date >= '{since}' AND created_date < '{until}'"
    }

    # ç™¼å‡º GET è«‹æ±‚
    response = requests.get(base_url, params=params)
    response.raise_for_status()
    data = response.json()

    # Debug ç”¨
    print(f"âœ… Fetched {len(data)} records from API.")

    return data

# === 2. é©—è­‰ + æ¸…æ´— ===
@task()
def validate_and_clean_data(json_data):
    df = pd.DataFrame(json_data)

    # Debugï¼šå…ˆçœ‹è³‡æ–™é•·ä»€éº¼æ¨£
    print(f"Raw data length: {len(df)}")
    print(f"Columns: {df.columns.tolist()}")
    print(df.head())

    if df.empty:
        raise AirflowSkipException("âœ… ç•¶æ—¥ç„¡è³‡æ–™ï¼Œç•¥éŽ validate èˆ‡å¾ŒçºŒå¯«å…¥éšŽæ®µ")

    required_columns = ['unique_key', 'created_date', 'complaint_type', 'descriptor', 'latitude', 'longitude']
    missing = [col for col in required_columns if col not in df.columns]
    if missing:
        raise ValueError(f"Missing columns: {missing}")

    df = df[required_columns].dropna()

    # åž‹åˆ¥è½‰æ›ï¼Œè½‰å¤±æ•—æ™‚ç‚º NaN
    # âœ… rebuilt schema å¾Œçš„åž‹åˆ¥è½‰æ›
    df['unique_key'] = pd.to_numeric(df['unique_key'], errors='coerce')  # è½‰æˆ BIGINT
    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')  # è½‰æˆ TIMESTAMP
    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')  # DOUBLE PRECISION
    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')


    df = df.dropna()

    return df.to_json(orient='records')

# === 3. Insert into Staging Table ===
@task()
def insert_into_staging_table(json_str):
    df = pd.read_json(json_str)
    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce', unit='ms')

    hook = PostgresHook(postgres_conn_id='postgres_default')
    conn = hook.get_conn()
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS stg_nyc_311 (
            unique_key BIGINT PRIMARY KEY,
            created_date TIMESTAMP,
            complaint_type VARCHAR(100),
            descriptor VARCHAR(100),
            latitude DOUBLE PRECISION,
            longitude DOUBLE PRECISION
        );
    """)
    conn.commit()

    for _, row in df.iterrows():
        cur.execute("""
            INSERT INTO stg_nyc_311 (unique_key, created_date, complaint_type, descriptor, latitude, longitude)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (unique_key) DO NOTHING;
        """, (
            row['unique_key'],
            row['created_date'],
            row['complaint_type'],
            row['descriptor'],
            row['latitude'],
            row['longitude']
        ))

    conn.commit()
    cur.close()
    conn.close()


# === 4. å»ºç«‹ Partitionï¼ˆday / week / monthï¼‰ ===
@task()
def create_partitions(partition_mode='week'):
    """
    å»ºç«‹åˆ†å€è³‡æ–™è¡¨ï¼šä¾æ“š dayã€weekã€month å»ºç«‹ RANGE åˆ†å€
    - é¿å…é‡ç–Šèˆ‡èªžæ³•éŒ¯èª¤
    - å­è¡¨ä¸é‡è¤‡å®£å‘Šæ¬„ä½èˆ‡ primary keyï¼ˆç”±æ¯è¡¨ç¹¼æ‰¿ï¼‰
    """

    context = get_current_context()
    execution_date = context["execution_date"].date()

    # === å»ºç«‹æ¯è¡¨ï¼ˆåªå»ºç«‹ä¸€æ¬¡ï¼‰===
    create_main_sql = """
    CREATE TABLE IF NOT EXISTS stg_nyc_311_partitioned (
        unique_key BIGINT PRIMARY KEY,
        created_date TIMESTAMP,
        complaint_type VARCHAR(100),
        descriptor VARCHAR(100),
        latitude DOUBLE PRECISION,
        longitude DOUBLE PRECISION,
        load_date DATE
    ) PARTITION BY RANGE (load_date);
    
    CREATE INDEX IF NOT EXISTS idx_stg_nyc_311_load_date
        ON stg_nyc_311_partitioned (load_date);
    """

    # === è¨ˆç®—åˆ†å€æ™‚é–“ç¯„åœ ===
    if partition_mode == 'day':
        start = execution_date
        end = start + timedelta(days=1)
        partition_name = f"stg_nyc_311_day_{start.strftime('%Y%m%d')}"

    elif partition_mode == 'week':
        start = execution_date - timedelta(days=execution_date.weekday())
        end = start + timedelta(days=7)
        partition_name = f"stg_nyc_311_week_{start.strftime('%Y%m%d')}"

    elif partition_mode == 'month':
        start = execution_date.replace(day=1)
        if start.month == 12:
            end = start.replace(year=start.year + 1, month=1)
        else:
            end = start.replace(month=start.month + 1)
        partition_name = f"stg_nyc_311_month_{start.strftime('%Y%m')}"

    else:
        raise ValueError("Invalid partition_mode. Must be one of: day, week, month")

    print(f"ðŸ”§ å»ºç«‹ partition: {partition_name} ({start} ~ {end})")

    # === å»ºç«‹åˆ†å€ SQL ===
    create_partition_sql = f"""
    DO $$
    BEGIN
        IF NOT EXISTS (
            SELECT FROM pg_tables WHERE schemaname = 'public' AND tablename = '{partition_name}'
        ) THEN
            EXECUTE format(
                'CREATE TABLE %I PARTITION OF stg_nyc_311_partitioned FOR VALUES FROM (%L) TO (%L);',
                '{partition_name}', '{start}', '{end}'
            );
            EXECUTE format(
                'CREATE INDEX %I ON %I (load_date);',
                'idx_{partition_name}_load_date', '{partition_name}'
            );
        END IF;
    END$$;
    """

    # === åŸ·è¡Œ SQL ===
    hook = PostgresHook(postgres_conn_id='postgres_default')
    hook.run(create_main_sql)
    hook.run(create_partition_sql)
    print("âœ… Partition å»ºç«‹å®Œæˆ")


# === 5. Insert into Partitioned Table ===
@task()
def insert_into_partitioned_table(json_str):

    df = pd.read_json(json_str)
    df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce', unit='ms')
    df['load_date'] = datetime.utcnow().date()

    hook = PostgresHook(postgres_conn_id='postgres_default')
    conn = hook.get_conn()
    cur = conn.cursor()

    failed = 0
    for _, row in df.iterrows():
        try:
            cur.execute("""
                INSERT INTO stg_nyc_311_partitioned (
                    unique_key, created_date, complaint_type, descriptor, latitude, longitude, load_date
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (unique_key) DO NOTHING;
            """, (
                row['unique_key'],
                row['created_date'],
                row['complaint_type'],
                row['descriptor'],
                row['latitude'],
                row['longitude'],
                row['load_date']
            ))
        except Exception as e:
            failed += 1
            print(f"âŒ Failed to insert row with unique_key={row['unique_key']}: {e}")

    conn.commit()
    cur.close()
    conn.close()
    print(f"âœ… Insert complete. Total rows: {len(df)}, Failed: {failed}")

# âœ… 4. åŸ·è¡Œ dbt run / dbt test

host_dbt_path = os.path.abspath("/Users/jerrychen/Documents/311/dbt")  # æ ¹æ“šä½ å°ˆæ¡ˆå¯¦éš›ä½ç½®, hostçš„çµ•å°è·¯å¾‘

run_dbt_dim_tables = DockerOperator(
    task_id='run_dbt_dim_tables',
    image='ghcr.io/dbt-labs/dbt-postgres:1.8.1',
    api_version='auto',
    auto_remove=True, # auto_remove=Trueï¼ˆæœƒè‡ªå‹•åˆªé™¤ containerï¼‰ï¼Œè‹¥é »ç¹åŸ·è¡Œï¼Œå¯è€ƒæ…®å…ˆé—œæŽ‰
    command='run --select marts.dim_*',
    working_dir='/usr/app',
    docker_url='unix://var/run/docker.sock',
    #network_mode='bridge',
    network_mode='311_default',
    mount_tmp_dir=False,
    mounts=[
        Mount(source=host_dbt_path, target='/usr/app', type='bind')
    ],
    environment={
        'DBT_PROFILES_DIR': '/usr/app'
    }
)

# æ¯æ—¥é€²è¡Œdbt run and dbt test (ä¹Ÿå¯ä»¥æ‰‹å‹•) 

# docker exec -it 311-dbt-1 bash
# dbt run
# dbt test

test_dbt = DockerOperator(
    task_id='test_dbt',
    image='ghcr.io/dbt-labs/dbt-postgres:1.8.1',
    api_version='auto',
    auto_remove=True,
    entrypoint=['/bin/bash', '-c'],
    command=[
        'mkdir -p logs && '
        'dbt test --select marts.dim_* | tee logs/dbt_test_$(date +"%Y%m%d_%H%M%S").log || true'  # åŒæ™‚å°‡ dbt test çš„è¼¸å‡ºå¯«å…¥å°æ‡‰æ™‚é–“å‘½åçš„ log æª”æ¡ˆ
    ],
    # command=['dbt test --select dim_* | tee logs/dbt_test_$(date +"%Y%m%d_%H%M%S").log || true'],
    # dbt test ä¸ç®¡çµæžœå¦‚ä½•éƒ½ return 0ï¼ˆè®“ Airflow åˆ¤å®šç‚ºæˆåŠŸ ï¼‰ã€‚
	# æ‰€æœ‰æ¸¬è©¦è­¦å‘Šèˆ‡éŒ¯èª¤æœƒå¯«å…¥ logï¼Œå¯è®“ä½¿ç”¨è€…åœ¨ Airflow UI ä¸­æª¢æŸ¥ã€‚
	# è‹¥è¦è‡ªå‹•åˆ†æžéŒ¯èª¤ï¼ˆä¾‹å¦‚ grep WARN æˆ– FAILï¼‰ï¼Œä¹Ÿèƒ½é€éŽ DockerOperator + XCom æˆ– log parsing å¯¦ç¾ã€‚
    working_dir='/usr/app',
    docker_url='unix://var/run/docker.sock',
    
    network_mode='311_default',
    mount_tmp_dir=False,
    mounts=[
        Mount(source=host_dbt_path, target='/usr/app', type='bind')
    ],
    environment={
        'DBT_PROFILES_DIR': '/usr/app'
    }
)

# dbt test çµæžœå¯«å…¥ log ä»¥ä¾›åˆ†æž 
scan_dbt_test_log = DockerOperator(
    task_id='scan_dbt_test_log',
    image='ghcr.io/dbt-labs/dbt-postgres:1.8.1',
    api_version='auto',
    auto_remove=True,
    entrypoint=['/bin/bash', '-c'],
    command=[
        'logs_dir=/usr/app/logs && '
        'latest=$(ls -t $logs_dir/dbt_test_*.log | head -n1) && '
        '[ -z "$latest" ] && echo "âš ï¸ No dbt_test log found. Skipping." && exit 0 || '
        'timestamp=$(basename "$latest" | sed "s/dbt_test_\\(.*\\)\\.log/\\1/") && '
        'summary=$logs_dir/dbt_test_summary_${timestamp}.log && '
        'warns=$(grep -c WARN "$latest" || true) && '
        'fails=$(grep -c FAIL "$latest" || true) && '
        'echo "# dbt test summary - generated by Airflow DAG" > "$summary" && '
        'echo "# Execution time: $timestamp" >> "$summary" && '
        'echo "# Total warnings: $warns" >> "$summary" && '
        'echo "# Total failures: $fails" >> "$summary" && '
        'echo "# Source log: $(basename "$latest")" >> "$summary" && '
        'grep -E "WARN|FAIL" "$latest" >> "$summary" || true && '
        'echo "ðŸ“ Summary written to: $summary"'
    ],
    working_dir='/usr/app',
    docker_url='unix://var/run/docker.sock',
    network_mode='311_default',
    mount_tmp_dir=False,
    mounts=[
        Mount(source=host_dbt_path, target='/usr/app', type='bind')
    ],
    environment={
        'DBT_PROFILES_DIR': '/usr/app'
    }
)


# âœ… DAG è¨­å®šèˆ‡ä»»å‹™ä¸²æŽ¥
with DAG(
    dag_id='nyc_311_etl_pipeline',
    default_args=default_args,
    schedule_interval='@daily',
    start_date=days_ago(1),
    catchup=True,
    tags=["etl", "airflow", "nyc311"]
) as dag:

    raw = fetch_data()
    cleaned = validate_and_clean_data(raw)
    staging = insert_into_staging_table(cleaned)
    create_parts = create_partitions()
    partitioned = insert_into_partitioned_table(cleaned)

    run_dbt_dim_tables.dag = dag
    test_dbt.dag = dag
    scan_dbt_test_log.dag = dag


    # æ›´å¼·çš„é †åºä¾è³´
    #	â€¢	å¼·åˆ¶è¦æ±‚è³‡æ–™å¿…é ˆå…ˆå¯«å…¥ stagingï¼Œä¹‹å¾Œæ‰é–‹å§‹å»ºç«‹ partitionã€‚
    #	â€¢	æ¯”åŽŸæœ¬ [staging, create_parts] ä¸¦è¡Œé‚è¼¯æ›´ç©©å®šï¼Œé¿å…æ½›åœ¨çš„ race conditionï¼ˆè¬ä¸€ partition æ¯” insert æ™šæ‰å»ºå°± GGï¼‰

    raw >> cleaned >> staging >> create_parts >> partitioned >> run_dbt_dim_tables >> test_dbt >> scan_dbt_test_log



