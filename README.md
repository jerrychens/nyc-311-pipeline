# ğŸ—½ NYC 311 ETL Pipeline with Airflow, dbt, and PostgreSQL

This is a modern data engineering project that builds an end-to-end ETL pipeline using **Apache Airflow**, **dbt**, **PostgreSQL**, and **Docker** â€” all orchestrated in a fully containerized environment.

ğŸ“Œ Use case: Ingest and model NYC 311 complaint data for analytical and monitoring purposes.

---

## ğŸ“Š Project Overview

| Component    | Description |
|-------------|-------------|
| **Airflow DAG** | Ingests 311 service request data from NYC Open Data API, validates and transforms it |
| **PostgreSQL** | Stores raw and modeled data with partitioning and indexing for performance |
| **dbt models** | Generates clean dimensional tables (`dim_complaint_type`, etc.) and analytical views |
| **Validation & Logging** | dbt tests + partitioned staging + incremental loads |
| **Docker** | All components run in isolated containers (Airflow, dbt, PostgreSQL) |

---

## ğŸš€ Quickstart (One-Click Setup)

1. Install **Docker Desktop**
2. Clone this repository and create a `.env` file:

```bash
git clone https://github.com/<your-username>/nyc-311-pipeline.git
cd nyc-311-pipeline
cp .env.example .env
```

3. Initialize & launch the full pipeline:

```bash
make init     # Initializes Airflow and pulls required images
make start    # Starts all services (webserver, scheduler, dbt, postgres)
```

4. Open Airflow UI at http://localhost:8080  
   (Login: `airflow / airflow`)

---

## ğŸ“… Running Backfill

To backfill historical data between a date range:

```bash
make backfill START=2025-05-01 END=2025-05-10
```

---

## ğŸ§± Folder Structure

```
.
â”œâ”€â”€ dags/                     # Airflow DAGs
â”œâ”€â”€ dbt/                      # dbt project (models, tests, profiles.yml)
â”œâ”€â”€ benchmark/                # Performance test SQLs and markdown
â”œâ”€â”€ logs/                     # Airflow logs
â”œâ”€â”€ docker-compose.yml        # Docker config
â”œâ”€â”€ Makefile                  # Easy CLI commands
â”œâ”€â”€ init.sh                   # Auto-bootstrap script
â””â”€â”€ README.md
```

---

## ğŸ§ª Data Quality & Testing

- âœ… **dbt test** results are stored in logs and summarized after each run
- ğŸ§¼ Records are validated before insertion via Airflow
- ğŸ” Partitioning is done by `load_date`, with optional `day`, `week`, or `month` mode

---

## ğŸ“ˆ Performance Benchmark (with/without Index)

Check [`benchmark/benchmark_result.md`](./benchmark/benchmark_result.md) for before/after query latency comparisons using various indexing strategies.

---

## ğŸ“¦ Deployment Environment

- Apache Airflow 2.7.2 (LocalExecutor)
- PostgreSQL 13
- dbt-postgres 1.8.1
- Docker Compose (v3.7)

---

## ğŸ› ï¸ Makefile Commands

| Command             | Description                        |
|---------------------|------------------------------------|
| `make init`         | Initialize Airflow setup           |
| `make start`        | Start all containers               |
| `make stop`         | Stop all services                  |
| `make backfill`     | Run historical backfill via DAG    |
| `make logs`         | Tail Airflow logs                  |
| `make clean`        | Remove all containers/volumes      |

---

## ğŸ§¾ License

MIT License Â© 2025 Jerry Chen

---

> Built with â¤ï¸ for learning, testing, and data-driven storytelling.
